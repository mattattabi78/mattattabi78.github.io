---
layout: post
title: SAM1(2023)
paper: "Segment Anything"
category: computer-vision
summary: paper reivew of SAM1
link: https://arxiv.org/pdf/2304.02643
---

# Type of Segmentation (Recap)

Segmentation is one of the main tasks in computer vision, and its importance is increasing with the growth of autonomous vehicles and robotics.  
The purpose of segmentation is to **assign each pixel to a class**, and there are various output types.

---

## Main Types of Segmentation
![image.png](/assets/images/SAM1/image.png)

### 1. Semantic Segmentation
Assigns a class label to every pixel in an image.  
All objects in the same category share the same label, without distinguishing individual instances.

### 2. Instance Segmentation
Classifies each pixel and **distinguishes between individual object instances** within the same class.

### 3. Panoptic Segmentation
Combines semantic and instance segmentation into a single framework.  
Every pixel receives **exactly one label**, either a stuff class or an instance ID.

### 4. Part Segmentation
Decomposes objects into their **semantic parts**, rather than treating objects as individual units.

---

## Abstract

The **SAM model** introduces a new task: *promptable segmentation*.  
> “In this work, our goal is to build a foundation model for image segmentation.”

The model uses this task as both **pre-training objective** and a method for **general downstream segmentation** via prompt engineering.

---

## Data Engine

Many foundation models use web-scale data, but segmentation masks are limited.  
SAM introduces a **model-in-the-loop dataset annotation**:

1. **Assisted Manual:** SAM helps annotators generate masks interactively (points or boxes). High-quality masks are collected.
2. **Semi-Automatic:** SAM generates masks automatically for some objects. Annotators focus on missing/wrong masks.
3. **Fully Automatic:** SAM prompts a regular grid of points, generating ~100 masks per image.

> Annotation cost is reduced because **prompt-level annotations** are cheaper than pixel-level.

![Data Engine Example](/assets/images/SAM1/스크린샷_2026-01-02_10.43.50.png)

---

## Task: Promptable Segmentation

Point-based segmentation can be **ambiguous**: e.g., a point on a person’s shirt could mean "shirt" or "person".  
SAM predicts **multiple masks per prompt**, making segmentation more flexible and transferable.

> A *prompt* is any user-provided signal specifying what to segment: point, box, low-res mask, text.

---

## SAM Architecture

SAM has three components:  

1. **Image Encoder**: Extracts embeddings from the input image. Uses ViT + MAE, input resolution $1024 \times 1024$, embedding $64 \times 64$.
2. **Prompt Encoder**: Encodes sparse (points/boxes/text) and dense (rough masks) prompts.
3. **Mask Decoder (Lightweight)**: Uses **self- and cross-attention** to generate masks per prompt.

![SAM Architecture](/assets/images/SAM1/스크린샷_2026-01-06_19.16.09.png)

- Output tokens → MLP → each pixel classified as part of mask  
- Multiple output tokens for ambiguous prompts

---

## Loss & Training

Linear combination of **Focal Loss** and **Dice Loss**:

$$
L_{\mathrm{total}} = \lambda L_{\mathrm{focal}} + (1-\lambda) L_{\mathrm{dice}}
$$

- **Focal Loss** focuses on edge pixels:

$$
L_{\mathrm{focal}} = -\alpha (1-p_t)^\gamma \log(p_t)
$$

- **Dice Loss** measures overlap between prediction and ground truth:

$$
\mathrm{Dice}(P,G)= \frac{|P| + |G|}{2|P \cap G|}, \quad
L_{\mathrm{dice}} = 1 - \mathrm{Dice}(P, G)
$$

---

## Limitation

- May miss fine structures  
- Can hallucinate small disconnected components  
- Struggles on zoomed images  
- Designing simple prompts for semantic/panoptic segmentation is non-trivial

---

