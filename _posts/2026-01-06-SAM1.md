---
layout: post
title: SAM1
paper: "Segment Anything"
category: computer-vision
summary: paper reivew of SAM1
link: https://arxiv.org/pdf/2304.02643
---


## Type of segmentation(recap)

![image.png](assets/SAM1/image.png)

Segmentation is one of main task of computer vision and its importance is getting bigger with growing of Autonomous vehicle and robotics.

The purpose of segmentation can be expressed “device each pixel and located classes” and it is various as output types.

Let see main type of segmentation task before see the SAM paper.

### Semantic Segmentation

Semantic segmentation assign a class label to every pixel in an image. All objects that belong to the same category share the same label without distinguishing individual instance.

### Instance Segmentation

Instance segmentation not only classifies each pixel but also distinguishes between individual object instance within the same class.

### Panoptic Segmentation

Panoptic segmentation unifies semantic segmentation and instance segmentation in to a single consistent framework. Every pixel in the image receives exactly one label either a stuff class or an instance ID. In this state, we can say model understand scene completely.

### **Part Segmentation**

Part segmentation decomposes object into their semantic part, rather than treating objects as individual units.

## Abstract

Researcher introduce new model(SAM) and dataset for image segmentation task. SAM is designed to be promptable.

*“In this work, our goal is to build a foundation model for image segmentation.”* With this sentence, they define new task: promptable segmentation.

And use this task as both pre-training objective and to sole general downstream segmentation task via prompt engineering.

## Data engine

Many foundation model use web-scale data but mask data for segmentation are not abundant. So researcher built data engine. They develop model-in-the-loop dataset annotation.

![스크린샷 2026-01-02 오후 10.43.50.png](assets/SAM1/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2026-01-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_10.43.50.png)

Data engine has three stages.

- Assisted manual: SAM assists annotators in annotating mask like classic interactive segmentation setup(point or bounding box). In this stage. Model got high quality masked data and reasonable prior.

After sufficient data annotation, SAM was retrained using only newly annotated masks.

- Semi Automatic: SAM can automatically generate mask for a subset of objects by prompting it and annotators focus on annotating the remaining object. In this stage, only wrong examples be a data. Thus, dataset’s distribution is moved to ‘failure region’ gradually.
- Fully automatic: Prompt SAM with a regular grid of foreground points average 100 masks per image. In this stage, amount of data are exploding.

With this engine, annotation cost is amazingly low because ‘prompt’ level annotating is cheaper than classic segmentation annotating with pixel level.

![스크린샷 2026-01-02 오후 10.54.31.png](assets/SAM1/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2026-01-02_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_10.54.31.png)

## Task

There is ambiguity in point based segmentation. For example, point on person who wearing shirt can mean both shirt and person. To solve this ambiguity, they design SAM to predict multiple masks for a single prompt. This NLP style prompt based segmentation can lead to a natural pre-training algorithm and a general method for zero-shot transfer to downstream segmentation tasks via prompting.

### The meaning of promptable segmentation.

Classic segmentation’s training is exist in only closed ontology form because its task aims to detect objects. Thus even model detect new object in data, it is error because it is not in closed ontology. But SAM had different goals. It aimed to find separable region of data by given prompt which eventually means ‘object’.

In this flow, the PROMPT is any user-provided signal that specifies what to segment(point, box, low resolution mask, text)

즉 기존의 객체 탐지 모델들이 이미지에서 semantic 정보를 추출하고 그것을 기반으로 object의 종류를 classification하는 task를 가지고 있었다면, SAM은 애초에 classification이 아니라 객체 간의 경계선을 구분하는 방향으로 학습된다.

## SAM

SAM has three components: Image encoder, Prompt encoder and Mask decoder.

### Image Encoder

First they built MAE(Masked Auto Encoder) which ViT model as real-time performance. The image encoder runs once per image and can be applied prior to prompting the model.

- what is MAE?

Following standard practices, they use an input resolution of $1024 \times 1024$ obtained by padding and rescaling the image. The image embedding is therefore $64 \times 64$. To reduce channel dimension, use $1 \times 1$ conv to get 256 channels followed by a $3 \times 3$ conv also with 256 channels. Each conv is followed by a layer normalization.

### Prompt Endcoder

- sparse prompt(points, boxs, texts)

They represent point and box by positional encoding summed with learned embedding. And use CLIP Encoder to match text prompt and image.

- dense prompt(rough masks)

Mask prompt are embedded using convolutions and summed element wise with image embedding.

### Mask Decoder(light weight)

![스크린샷 2026-01-06 오후 7.16.09.png](assets/SAM1/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2026-01-06_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_7.16.09.png)

With two directions of self-attention and cross-attention(p2i and i2p), decoder block can capture mask from ambiguous prompt. Decoder maps Image embedding, Prompt embedding and output token(target mask).

Image & Prompt embedding, Output token → Transformer decoder(self & cross) → aligned image and prompt feature → Image embedding upsample → classify output token with MLP → get Mask!

여기서 Output token은 prompt에 대응하는 한 개의 마스크 정보가 요약된 벡터이다. 이때 Output token을 MLP를 이용해 하나의 prompt에 의존적인 분류기로 만들어준다. 이후 image embedding의 각 픽셀들이 이 분류기와의 내적을 통해 마스크에 포함될지 결정한다.

To capture masks from ambiguous prompt, model predict multiple output tokens.

### Loss and Training

Used linear combination of focal loss and dice loss.

**Linear combination of focal loss** is Loss policy in segmentation task.

$$
L_{\mathrm{total}} = \lambda L_{\mathrm{focal}} + ( 1-\lambda) L_{\mathrm{dice}}
$$

- Focal loss

$$
L_{\mathrm{focal}} = -\alpha(1-p_t)^γ \log(p_t)
$$

$p_t$: t번째 픽셀에 대한 정답 클래스의 예측 확률

$γ$: focusing parameter(normally 2)

Thus, this loss functioned more focus and sensitive on edge pixel.

- Dice loss(non-soft)

$$
\mathrm{Dice}(P,G)= \frac{∣P∣+∣G∣}{2∣P∩G∣}, \ \ \ \ \ L_{\mathrm{dice}} = 1 - \mathrm{Dice}(P, G)
$$

P: mask of prediction

G: mask of ground truth

## Limitation

It can miss fine structures, hallucinate small disconnected components at times, and can not segment zoomed image. While SAM can perform many tasks, it is unclear how to design simple prompts that implement semantic and panoptic segmentation.
