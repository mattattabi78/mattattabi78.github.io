---
layout: post
title: SAM1(2023)
paper: "Segment Anything"
category: computer-vision
summary: paper reivew of SAM1
link: https://arxiv.org/pdf/2304.02643
---

## Type of Segmentation (Recap)

Segmentation is one of the main tasks in computer vision, and its importance is increasing with the growth of autonomous vehicles and robotics.  
The purpose of segmentation is to **assign each pixel to a class**, and there are various output types.

---

## Main Types of Segmentation
![image.png](/assets/images/SAM1/image.png)

### 1. Semantic Segmentation
Assigns a class label to every pixel in an image.  
All objects in the same category share the same label, without distinguishing individual instances.

### 2. Instance Segmentation
Classifies each pixel and **distinguishes between individual object instances** within the same class.

### 3. Panoptic Segmentation
Combines semantic and instance segmentation into a single framework.  
Every pixel receives **exactly one label**, either a stuff class or an instance ID.

### 4. Part Segmentation
Decomposes objects into their **semantic parts**, rather than treating objects as individual units.

---

## Abstract

The **SAM model** introduces a new task: *promptable segmentation*.  
> “In this work, our goal is to build a foundation model for image segmentation.”

The model uses this task as both **pre-training objective** and a method for **general downstream segmentation** via prompt engineering.

---

## Data Engine

Many foundation models use web-scale data, but segmentation masks are limited.  
SAM introduces a **model-in-the-loop dataset annotation**:

1. **Assisted Manual:** SAM helps annotators generate masks interactively (points or boxes). High-quality masks are collected.
2. **Semi-Automatic:** SAM generates masks automatically for some objects. Annotators focus on missing/wrong masks.
3. **Fully Automatic:** SAM prompts a regular grid of points, generating ~100 masks per image.

> Annotation cost is reduced because **prompt-level annotations** are cheaper than pixel-level.

![Data Engine Example](/assets/images/SAM1/image2.png)

---

## Task: Promptable Segmentation

There is ambiguity in point based segmentation. For example, point on person who wearing shirt can mean both shirt and person. To solve this ambiguity, they design SAM to predict multiple masks for a single prompt. This NLP style prompt based segmentation can lead to a natural pre-training algorithm and a general method for zero-shot transfer to downstream segmentation tasks via prompting.

> A *prompt* is any user-provided signal specifying what to segment: point, box, low-res mask, text.

---

## SAM Architecture

![SAM Architecture](/assets/images/SAM1/image3.png)

SAM has three components:  

1. **Image Encoder**

First they built MAE(Masked Auto Encoder) which ViT model as real-time performance. The image encoder runs once per image and can be applied prior to   prompting the model.

- what is MAE?
Following standard practices, they use an input resolution of $$1024 \times 1024$$ obtained by padding and rescaling the image. The image embedding is therefore $$64 \times 64$$. To reduce channel dimension, use $$1 \times 1$$ conv to get 256 channels followed by a $$3 \times 3$$ conv also with 256 channels. Each conv is followed by a layer normalization.

   
3. **Prompt Encoder**
- sparse prompt(points, boxs, texts)
They represent point and box by positional encoding summed with learned embedding. And use CLIP Encoder to match text prompt and image.

- dense prompt(rough masks)
Mask prompt are embedded using convolutions and summed element wise with image embedding.


4. **Mask Decoder**:

With two directions of self-attention and cross-attention(p2i and i2p), decoder block can capture mask from ambiguous prompt. Decoder maps Image embedding, Prompt embedding and output token(target mask).

Image & Prompt embedding, Output token → Transformer decoder(self & cross) → aligned image and prompt feature → Image embedding upsample → classify output token with MLP → get Mask!

여기서 Output token은 prompt에 대응하는 한 개의 마스크 정보가 요약된 벡터이다. 이때 Output token을 MLP를 이용해 하나의 prompt에 의존적인 분류기로 만들어준다. 이후 image embedding의 각 픽셀들이 이 분류기와의 내적을 통해 마스크에 포함될지 결정한다.

To capture masks from ambiguous prompt, model predict multiple output tokens.



- Output tokens → MLP → each pixel classified as part of mask  
- Multiple output tokens for ambiguous prompts

---

## Loss & Training

Linear combination of **Focal Loss** and **Dice Loss**:

$$
L_{\mathrm{total}} = \lambda L_{\mathrm{focal}} + (1-\lambda) L_{\mathrm{dice}}
$$

- **Focal Loss** focuses on edge pixels:

$$
L_{\mathrm{focal}} = -\alpha (1-p_t)^\gamma \log(p_t)
$$

- **Dice Loss** measures overlap between prediction and ground truth:

$$
\mathrm{Dice}(P,G)= \frac{|P| + |G|}{2|P \cap G|}, \quad
L_{\mathrm{dice}} = 1 - \mathrm{Dice}(P, G)
$$

---

## Limitation

- May miss fine structures  
- Can hallucinate small disconnected components  
- Struggles on zoomed images  
- Designing simple prompts for semantic/panoptic segmentation is non-trivial

---

