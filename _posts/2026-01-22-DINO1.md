---
layout: post
title: DINO1 paper review
paper: "Emerging Properties in Self-Supervised Vision Transformers(2021)"
category: computer-vision
summary: paper reivew of DINO1
link: https://arxiv.org/pdf/2104.14294
---



Main observation of this paper is 
1:  self-supervised ViT features contain explicit information about the semantic segmentation of an images. 
2: these features are also excellent k-NN classifiers. In the easiest expression, DINO train good Encoder with self-distillation and constrastive learning.
And researcher introduce DINO, which interpret image to K-dimension feature vector as a form of self-distillation with no labels.

요약하면, DINO는 제한된 모델 사이즈 내에서 모멘텀을 갖는 teacher 모델과 즉각적인 gradient에 적응하는 student 모델을 적절하게 사용하여 대부분의 데이터에 대해 일관된 표현 공간을 공유하도록 하는 모델이다. (즉, 여기서 정의하는 좋은 feature란 이미지에 관계 없이 비슷한 semantic은 비슷하게 표현하는 것이다.)

## Background

### Momentum Encoder(from MoCo: Momentum Contrast)
**What is constrative learning?**
constrastive learning(대조학습)이란 Self-Supervised Visual Representation Learning 방법 중 하나로,  모델이 어떤 두 샘플에 대해 같은 semantic을 갖는지 구분하도록 학습시켜 “의미적으로 잘 구조화된 표현”을 얻는 것을 목표로 하는 학습 방법이다. Embedding space에서 같은 semantic을 공유하는 Positive pair의 거리는 가깝게, 다른 semantic을 갖는 Negative pair의 거리는 멀게 위치하도록 contrastive loss를 통해 수식으로 강제한다. 이 학습 방법은 라벨 없이도 학습이 가능한 self-supervised learning의 기초가 되며, 단순 분류 방식보다 개념적 구조를 잘 포착한다.
    
**Dynamic Dictionary**

Constrative learning에서는 모델이 대조할 수 있는 negative sample이 필수적이다. 그러나 미니 배치 학습에서는 negative sample 개수가 배치 크기에 제한되기 때문에 해당 논문에선 FIFO를 이용해 Dictionary 형태로 과거 배치에서 계산된 key embedding을 저장해둔다. 이러한 Dictionary는 학습 과정 중 동적으로 변화한다.

**Query Encoder**

학습 과정에서 이미지를 feature space로 embedding하는 인코더. backprop(SGD)를 통해 업데이트되며 파라미터 업데이트의 의미적 변화 속도가 빠르다.

**Momentum(Key) Encoder**

그러나 인코더 역시 학습 과정에서 가중치가 업데이트되기 때문에 Dictionary에서 Key를 가져오더라도 현재 인코더의 representation space에 적용 불가능한 문제인 representation drift가 발생한다.

따라서 이 추가적인 encoder는 EMA를 사용하여 가중치를 업데이트하는 방식으로 파라미터 업데이트의 의미적 변화 속도를 줄인다.

즉, 현재 네트워크를 직접 학습시키지 않고 메인 encoder의 가중치를 점진적으로 모방하도록 업데이트하는 보조 encoder이다.
    
### self-supervised learning
    
사람이 직접 라벨을 다는 것이 아니라 모델이 스스로 개념을 구분하도록 유도하는 학습 방법. 모델 사이즈와 비례하여 이해 가능한 수준이 커짐에 따라 사람이 모든 개념을 정의하기 어렵기 때문에 최근 대규모 foundation model의 핵심 학습 전략으로 쓰인다.
그러나 SSL의 ‘같은 것은 같게’ $$\underset{f}{\mathrm{min}} \ d(f(x_1), f(x_2))$$방식의 접근은 필연적으로 model collapse를 ehd반한다. 따라서 contrastive learning(다른 것은 다르게), stop-grad + predictor, balance clustering 등의 방법이 사용된다.

### multi-crop agmentation
한 이미지에서 서로 다른 해상도의 여러 crop을 동시에 만들어서 모델이 스케일 불변적이고 의미 중심적인 표현을 배우게 하는 데이터 증강 기법.
    

![image.png](/assets/images/DINO1/1.png)

Self attention from a Vision Transformer with no supervision. Look at the self-attention of the CLS token on the heads of the last layer. These maps show the model automatically learns class-specific features leading to unsupervised object segmentations.

Researcher said self-supervised ViT features explicitly contain the object boundary and well with a basic k-NN without ant finetuning.

![image.png](/assets/images/DINO/2.png)

## Self supervised Learning with Knowledge distillation

Let student network $$g_{\theta_s}$$ and teacher network $$g_{\theta_t}$$ and parameterized by $$\theta_s$$ and $$\theta_t$$ respectively. Given input image $x$, both networks output probability distribution over $K$ dimension denoted by $$P_s$$ and $$P_t$$ with normalized with softmax function.

<aside>

They share architecture(embedding dim, num of layers, num of head) and size of parameters but Teacher model use momentum update(EMA). In this point DINO’s distillation is not convey inductive bias, but more like self-alignment to prevent model collapse.

</aside>

Network $$g_{\theta_t}$$ learn to match these distributions by minimizing the cross-entropy loss the parameters of the student network $$\theta_s$$ with equation below.

$$
\underset{\theta_s}{\mathrm{min}} \ H(P_t(x), P_s(x))
$$

Generate a set $$V$$ of different view which contain two global views $$x^g_1$$, $$x^g_2$$ and several local view of given image. All crops are passed through the student while only the global views are passed through the teacher.

설계적으로 teacher model은 student model의 과거 지수 이동 평균(EMA)이다. 따라서 일반적인 Knowlege distillation과는 다르게 student 모델은 자기 자신의 과거 feature embedding 을 참고하면서 일관된 feature 방향을 학습하게 된다. 이 과정에서 encoder는 객체 중심의 semantic structure를 갖게 된다.

### Teacher Network

Unlike knowledge distillation, teacher network using EMA(exponential moving average) on the student weight. The update rule is $$\theta_t  ← \lambda \theta_t + (1-\lambda)\theta_s$$ with $$\lambda$$ following a cosine schedule from 0.998 to 1 during training.

### Network Architecture

Network $$g$$ is composed of a backbone $$f$$(ViT or ResNet) and of a projection head $$h: g = h ◦ f$$ where $$h$$ consists of a 3 layer MLP with 2048 dim by $$l_2$$ norm and weight norm with $$K$$ dim

### Avoiding collapse

DINO use both centering and sharpening for balancing between one dimension to dominate and uniform distribution. And here is what is centering and sharpening. By doing this, output vector are stable and meaningful at the same time. It can be represented by below.

$$
c ← mc + (1-m)\frac{1}{B} \overset{B}{ \underset{i=1}{ \sum}}g_{\theta_t}(x_i)
$$

**Centering**: minus mean value of output vector in batch → balancing between elements of feature vector and prevent one dim domination problem.

**Sharpening**: amplify small difference between elements of feature vector.

### What is Projection Heads?

Projection head is MLP that separates model’s training space and representation space. To put it simply, projection head is kind of buffer layer between loss flow and backbone. When constrastive loss directly flows to backbone, it can cause reducing of representation variance and linear probe quality. 

Thus, it provides a soft inductive bias by modifying the geometry of gradient flow rather than enforcing hard constraints.

But this is based on empirical observe.

### How student network trained?

Let student network and teacher network with projection head each $$z_s = g(f_{\theta_s}(x_s))$$ and $$z_t = g(f_{\theta_t}(x_t))$$  where $g$ is projection head.

Student distribution is..

$$
p^{(i)}_s = \frac{\exp(z^{(i)}_s / \tau_s)}{\sum_j \exp(z^{(i)}_s / \tau_s)}
$$

And Teacher distribution is…

$$
p^{(i)}_t = \frac{\exp((z^{(i)}_t - c^{(i)}) / \tau_t)}{\sum_j \exp((z^{(i)}_t - c^{(j)} / \tau_t)}
$$

$$\tau_s$$: student temperature(usually high) → smooth distribution

$$\tau_t$$: teacher temperature(usually low) → sharp distribution

$$c$$: centering

And loss function is like below

$$
\mathcal{L} = - \underset{i}{\sum} p_t^{(i)} \log p_s^{(i)}
$$

By doing this, student network can imitate teacher network and teacher network’s parameters are EMA of student network’s parameters

projection head가 없으면 학습 과정에서 constastive loss가 그대로 backbone의 파라미터에 영향을 주게 되어 학습 이후 표현력이 극히 제한적으로 바뀐다. 따라서 그 사이에 Projection Head를 삽입하면 이러한 효과를 완충하여 backbone model이 오로지 의미 있는 표현만 배울 수 있게 한다.

It improves the accuracy of DINO with n-layer MLP - 2048d and GELU activation func.

