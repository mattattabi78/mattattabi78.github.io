---
layout: post
title: "SAM 3: Segment Anything with Concepts"
paper: "SAM 3: Segment Anything with Concepts(2025) - Meta"
category: computer-vision
summary: paper reivew of SAM3
link: https://arxiv.org/pdf/2511.16719
---

Image segmentation 분야에서 가장 최신이자 진보된 모델인 SAM 모델 논문을 차례대로 읽어보면서 그 흐름을 따라갈 수 있었다. 기존 SAM 모델들이 오브젝트의 경계를 탐지하는데 집중했다면, SAM3는 자연어 프롬프트를 통해 장면에서 시각적 개념과 해당 개념의 시각적 경계를 연결함으로써 인간의 시각 체계와 언어 개념 체계를 연결한 모델으로 볼 수 있다. 

특히 개인적으로 presence token을 사용한 부분이 인상적이었는데, 입력 영상의 객체 존재 여부를 단순 어텐션 메커니즘에 위임하기 어려웠기 때문에 특수한 토큰을 사용한 것이 똑똑하다고 생각이 들었다. 물론, 이러한 특수 토큰을 남발하면 모델이 완벽히 task를 이해했다고 보기 어렵겠지만, 일종의 technical detour처럼 적절한 상황에서 쓰면 좋은 것 같다.


SAM3 is advanced version of SAM2, which can treat both images and video domain, with based on concept *text prompt*. But algorithms of detecting object(concept) is quite different with previous version of SAMs. Amazingly, SAM3 doubles the accuracy of existing systems in both image and video PCS(Promptable Concept Segmentation).
While SAM3 is not designed for long referring expressions or queries requiring reasoning, they show that it can be straightforwardly combined with a Multimodal Large Language Model to handle more complex language prompts.

# Segment Anything with Concepts(SA-Co)

Researcher introduce new benchmark of PCS; SA-Co which containing 207K unique concepts with 120K images and 1.7k videos, not only objects but also extended-concepts(attribution or abstracted concepts).

### New task: Promptable Concept Segmentation(PCS)

Promptable Concept Segmentation task treat *concepts.* Differ from SAM1&2, SAM3 is targeted understand concept of prompt, not the boundary of object. It is following effects. PCS task allow form of input as simple noun phrase, consisting of a noun and optional modifiers. Also PCS stimulate to understand scene(image & short video) by keeping ambiguity of concept. 

By doing this, SAM3 allows open-vocabulary, multi-form of input and class-free detecting.

So, purpose of PSC is detecting all instance of applicable visual concept in scene with given short text, image or combination prompts.

## Model

![image1.png](/assets/images/SAM3/1.png)

It takes concept prompts or visual prompts to define the object to be individually segmented spatio-temporally.

![image2.png](/assets/images/SAM3/2.png)

### Detector Architecture

The architecture of the detector follows the general DETR paradigm. 

**What is DETR(DEtection TRansformer)?**

Given prompt tokens(embedded image example or text prompt), Fusion encoder refine image with emphasizing feature of prompt tokens with cross attending. Each DETR-like decoder layer find object(predict binary label of whether the object corresponds to the prompt) and delta from the bounding box predicted by previous level with hint from box-region-positional bias. In this process, DAC-DETR and Align loss are used for stabilize performance in generating mask with mask head with Semantic segmentation head.

Transformer로 객체를 한 번에 예측하는 object detecting transformer. 기존의 방식이 anchor box나 NMS(Non-Max Suppression)이 필요했다면 DETR는 고정된 object 개수 안에서 각 query가 객체 하나를 담당하여 end-to-end로 학습하는 모델이다.
기존 YOLO등의 방식이 위치(anchor)를 기준으로 근처에 객체 있는지를 파악한다면, DETR는 학습 과정에서 하나의 query에 object에 해당하는 개념이 할당된다.
그러나 YOLO와는 다르게 학습이 느리고 데이터 의존적이라는 한계가 있으며 탐지 가능한 object 수가 model query 수에 의존적이다.
    
즉 Detector 구조에서는 다음과 같은 과정을 거치게 된다.


 > text prompt → 개념 추출 → 이미지를 추출된 개념 기준으로 재해석 → DETR를 통해 object queries 생성 → 프롬프트와 맞는 객체 선택 & 픽셀 단위 개념 영역 에측


### Presence Token $$p \in \mathbb R^{d}$$

Presence token checks that the concept from noun-phrase prompt is exist in the input images. Not like DETR and previous version of SAM(These models presuppose there is some object in images.), SAM3 have to detect exact concept with given prompts.
It present existence directly with seeing whole patch of images and cross attention with possible object set. By doing this, query of model only needs to solve the localization problem.
(뭔가 이렇게 모델 아키텍처 내에서 명확히 역할을 할당하기 위해 특수 token을 부여하는게 신기한 것 같다. - positional encoding처럼)

### Image Exemplar

It is kind of concept anchor of image. Each image exemplar is encoded separately by the exemplar encoder using an embedding for the position, label, visual features, then concatenated and processed by a small transformer. The result of prompt is concatenated to the text prompt to comprise the prompt tokens.
Once image exemplar through encoder and concatenated with prompt, queries of non-related object is killed in cross-attention between image and prompts.
예를 들어, 사용자가 박스 형태로 exemplar를 제시하면 exemplar encoder는 rough mask 집합에 해당하는 masklet에서 해당 영역의 시각 패턴만 인코딩한 후 prompt token에 반영하게 되면 이미지와 프롬프트 간의 cross attention 시에 exemplar와 관련된 object의 query만 활성화 되는 것.

### Multimodal Decoder(fusion encoder)
Multimodal decoder receive various domain prompt(text, image exemplar) and cross attending these domains to print aligned image features(conditional frame embedding). It is stack of 6 transformer blocks with self- and cross attention(with prompt tokens).
즉 Detector 안에서 여러 쓸모 있는 정보를 받아서 detector decoder가 사용할 수 있도록 정보들을 정렬해주는 역할

### Detector Decoder(DETR decoder)
By using **object queries**, it produce masklet with output of multimodal decoder with cross attention and detector queries + presence token.
Each object query attends to the image tokens to generate an object-centric representation, which is then decoded into a pixel-level mask.(each query acting like 'slot')

Object queries are encouraged to represent semantic objects, BUT, the correspondence is not strictly constrained to represent a single semantic object.(remind there is no architecture evidence to constrain query to correspond single object)
Thus, in SAM3, obhject query works like semantic filters, not the classifer.

## Detector & Tracker

In frame t, Detector finds new objects $$\mathcal{O}_t$$, and the tracker propagates masklet $$\mathcal{M}_{t-1}$$ to $$\hat{\mathcal{M}}_t$$. And IoU based matching function to associate propagated masklets $$\hat{\mathcal{M}}_t$$ with new object mask emerging in the current frame $$\mathcal{O}_t$$ to $$\mathcal{M}_{t}$$.

## Data Engine

To achieve text prompt segmentation, SAM3’s data engine include three key ways.

- **Media curation**: Curate more diverse media domains.
    
- **Label curation**: Increase label diversity and difficulty by leveraging an ontology and muti-modal LLM as “AI annotators”
    
- **Label verification**: Double annotation throughput by fine-tuning MLLMs to be effective “AI verifiers” that achieve near-human accuracy.
    

## Limitation of SAM3

- 복잡한 단어 조합에 대한 적응력이 떨어진다.
- 여전히 real-time으로 돌리기에는 무리이다.
- 마스크의 정보가 되는 concept들이 개별 국한되어 있으며, 복합적인 concept 간의 관계나 조합을 파악할 수 없다.
- t 범용적인 context memory가 없기 때문에 object가 사라졌다 다시 등장할 때 다른 개념으로 취급됨.
