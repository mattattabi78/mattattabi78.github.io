---
layout: post
title: "DINO3 paper review"
paper: "DINOv3(2025) - Meta"
category: computer-vision
summary: paper reivew of DINO3
link: https://arxiv.org/pdf/2508.10104
---

Authors leverage the benefit of scaling both dataset and model size and invent Gram anchoring to solve issue of degrading of dense feature maps in long term training. And they adjust model flexibility with post-hoc strategies. As a result DINOv3, SOTA versatile vision foundation model on various vision tasks.

## Problem setting of scaling SSL foundation model

1. Unclear how to collect useful data from unlabeled collection.

2. Hard to know proper step to using cosine schedules.

3. The performance of the feature gradually decreases after early training.

<img src="/assets/images/DINO3/figure1.png" class="img-small" alt="Figure 1">
This graph show SSL has quickly progressed and now reached performance of SL or WSL(Weakly supervised learning)

At this flow of SSL, DINOv3 has three purposes.

1. Strong & Versatile Foundation model: DINOv3 aims to offer versatility of scaling model size and training data.

2. Superior Feature maps Through Gram Anchoring: DINOv3 mitigate collapse of SSL by using Gram Anchoring and keep high quality of dense feature map, staying clean even high resolution.

3. The DINOv3 Family of Models: With 7B teacher across a variety of task and distillation methods, author show various student models which perfectly adjust in unique vision tasks.

<img src="/assets/images/DINO3/figure2.png" class="img-small" alt="Figure 2">
Comparison of the teacher architectures used in DINOv2 and DINOv3

# Scheme of DINO3

## Axial RoPE
Authors believed Self-supervised learning can produce high-quality visual features when scaling-up the model capacity like large language models. Authors increase model size to 7B parameters by defining a custom variant of the ViT architecture with axial RoPE.

**what is RoPE and axial RoPE?**
 RoPE(Rotary Positional Encoding)은 기존에 사인, 코사인 함수를 이용해 위치별 벡터를 더하는 PE 방식으로부터 발생하는 토큰 간 relative position 정보가 반영되지 않는 문제를 해결한다. 기존의 덧셈 방식의 PE 역시 이론적으로는 attention module 내에서 위치 간의 관계를 유추할 수 있지만, 구조적으로 명시되어 있지 않기 때문에 실제로는 긴 시퀀스에서 relative position 정보를 포착하지 못하는 경우가 많다. 이러한 문제를 RoPE는 Query, Key 벡터 자체를 위치에 따라 다르게 회전시킴으로써 relative position 정보를 전달한다.
 이 방식은 기존의 덧셈 방식보다 계산이 효율적이며 transformer 구조 수정 없이도 적용 가능하다. 또한 연속적이며 동시에 무한하기 때문에 dense feature map에도 쉽게 확장이 가능하다. 이러한 속성은 기존 PE 방식의 고정된 해상도에서만 작동하는 문제를 해결해주기도 한다.
 axial RoPE는 기존의 1-D RoPE의 한계를 뛰어넘어 이미지 도메인에서까지 RoPE를 적용하기 위해 N차원에 대한 각 축마다 독립적으로 RoPE를 적용하는 방식이다. 이러한 방식은 patch 간의 상대적 관계 의미를 중요시하는 dense feature를 사용하는 task (e.g segmentation, correspondence 등)에서 큰 효과를 보여준다.
    

<img src="/assets/images/DINO3/figure3.png" class="img-medium" alt="Figure 3">
performance of DINOv3


## Data curation

Data curation refers to the strategy of selecting and structuring large-scale web data based on diversity, balance and usefulness rather than using raw data indiscriminately. DINOv3 starts from the premise that simply scaling data does not guarantee better performance, and therefore combines clustering-based curation(to ensure conceptual diversity) with retrieval-based curation(to improve downstream task relevance).

In addition, high-quality curated dataset such as ImageNet are mixed in at a controlled ratio to balance generalization and task-specific performance.

즉, 대규모 웹 데이터를 무분별하게 학습에 넣는게 아니라 curating을 통해 매 배치마다 균형 있는 데이터를 넣겠다는 뜻.

And they mix different data parts together.

Homogeneous Batch: A batch composed of samples from a single dataset(e.g. ImageNet) → providing high-quality and consistent training signals.

Heterogeneous Batch: A batch composed of samples mixed from multiple data source, enhancing conceptual diversity and generalization.

DINOv3 randomly sample in each iteration either a homogeneous batch from ImageNet1k alone or a heterogeneous batch mixing data from all other components.


## Gram Anchoring

Authors analyze the loss of patch-level consistency and propose Gram Anchoring to mitigate  degrade of performance on dense tasks.

This phenomenon based on CLS token in ViT models. Authors figure it out that long-term training cause dominance of CLS which influence to patches.

> “We notice that the cosine similarity between CLS token and the patch output gradually increase during training.”

<img src="/assets/images/DINO3/figure4.png" class="img-medium" alt="Figure 4">
Degrading phenomenon of performance on dense task.

**Why the CLS token make this phenomenon?**
 SSL 학습의 핵심은 다른 background의 이미지에서도 같은 object의 CLS 표현이 같아지는 것이다. 이 과정에서 모델은 이미지의 어디를 보든 하나의 의미로 수렴하도록 강제되며 이 효과가 학습이 길어짐에 따라 patch feature 간 획일화를 유도하는 것이다. 이러한 획일화 과정에서 patch token들은 모든 patch를 참고하는 CLS token의 방향과 비슷해지고 locality가 떨어지게 된다.
    
But, exist method which combining global DINO loss with the local iBOT loss was unbalanced to solve this problem. So, they propose **new objective designed** to regularize the patch features and keep its consistency while preserving global performance.

This new loss function operates on the Gram Matrix of whole patches from image. The purpose is “pushing the Gram Matrix of the student towards Gram teacher”. By operating loss calculation on the Gram matrix rather than the feature themselves, the local features are free to move, provide the structure of similarity remains the same.

Let $$P$$ as patch size, $$d$$ as dimension size and denote $$\mathbf{X}_S$$ and $$\mathbf{X}_G$$ as the $$P \times d$$ matrix of $$L_2$$-normalized local features of the student and teacher. The loss of this can be represented by..

$$
\mathcal L_{\mathrm{Gram}} = || \mathbf{X}_S \mathbf{X}_S^{⊤} - \mathbf{X}_G \mathbf{X}_G^{⊤} ||^2_F
$$

This loss only computed in global crop and apply only after 1M iteration for considering computational cost. Lastly, Gram matrix is updated on each 10k iteration for additional performance. In this process new Gram teacher is identical with EMA teacher.

초기 teacher의 괜찮았던 패치 관계 구조를 Gram matrix 형태로 따로 보존하다가 학습 중간에 student의 Gram matrix와의 차이 행렬의 크기를 구해서 로스로 취급한다. 이를 통해 patch feature가 붕괴를 막을 수 있다.

feature 자체에 업데이트를 하는게 아니라 Gram matrix끼리 맞추는 과정을 통해 patch feature의 절대적인 값은 강제하지 않은채 patch 간의 상대 관계만 유지할 수 있다.

신기하게도 이미 망가진 patch feature들에 Gram anchor를 적용하더라도 feature들이 다시 되살아나는 것을 관찰했다고 한다.

추가로, 이런 Gram anchoring의 로스는 patch pixel 전체로 역전파되며 이 과정에서 자연스럽게 패치 간의 관계를 정렬시키게 된다.

## Leveraging Higher-Resolution Features

To produce more detailed feature maps, author double scaled Gram teacher of input resolution and $2 \times$down sample to fit in student output. They figured out that superior patch-level consistency is preserved through down sampling. In this process, by using RoPE, model can process images at varying resolutions.

Next, they calculate Gram matrix again with down sampled teacher features. This process looks like struggling to get better features by super-resolutioning feature maps which authors called “high-resolution refinement”.

<img src="/assets/images/DINO3/figure5.png" class="img-medium" alt="Figure 5">
Gram anchoring의 효과: 학습 초기 단계와 Gram anchoring refinement를 거친 후의 feature map 비교

## Model Distillation

Authors perform knowledge distillation of the ViT-7B model into smaller ViT(ViT-S, ViT-B, and ViT-L). But this time, instead of relying on an EMA, they use the 7B model directly as the teacher to guide the smaller student model. In this process they did not apply Gram anchoring and use effective multi-student distillation.

<img src="/assets/images/DINO3/figure6.png" class="img-medium" alt="Figure 6">

stability of the feature at multiple resolution for the DINOv3 ViT family of models
