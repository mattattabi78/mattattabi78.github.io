---
layout: post
title: Dreamer paper reivew
paper: "DREAM TO CONTROL: LEARNING BEHAVIORS BY LATENT IMAGINATION (ICLR 2020) - Google brain & Deepmind"
category: generative-ai
link: https://arxiv.org/pdf/1912.01603
summary: Summary of Dreamer paper!
---

# The Dreamer

Authors present Dreamer, an agent that learns long-horizon behaviors from images urely by latent imagination.

Dreamer learns a world model from past experience and efficiently learns farsighted behaviors in its latent space by backpropagating value estimates back through imagined trajectories.

저자는 해당 논문을 통해 강화 학습 에이전트에 이미지 기반 환경에서 복잡한 행동을 학습할 때 latent space에서의 latent imagination을 이용하는 방법을 제안한다. 기존의 world model들과는 다르게 인코더를 이용해서 모든 observe와 action을 latent state로 표현하고 latent space에서 다음 상태와 행동을 예측한다. 
즉 강화 학습 에이전트를 위한 효율적인 가상 시뮬레이션 환경이라는 것. 효율적인 world 이해를 위해 world의 상태를 임베딩, 예측, 평가하는 latent dynamic 부분과 action을 예측하고 평가하는 모델이 별도로 구분되어 있다. 이러한 방식은 다른 말로 Recurrent State-Space Model라고도 불린다.
이렇게 latent space에서 state와 추측하고 action을 정하기 때문에 기존 방식과는 다르게 action까지 backpropagation을 사용가능하다.

### Limitation of Previous work

Reinforcement learning agent’s long-term memory issue is unsolved problem because of the interaction between RL agent and real world require expensive computational costs. 

World model can solve this issue with imagination, but previous world models have short finite imagination horizon.

More specifically, in high dimensional observation like image, the efficiency of RL agent’s understanding is considered with its context understanding issues. 

### The key contribution of this paper

1. Learning long-horizon behaviors by latent imagination with predicting both actions and state values.(latent imagenation: 내부의 학습된 세계 모델을 상상하는 행위)

2. Authors paired Dreamer with existing representation learning method and evaluate it on the DeepMind Control Suite with image inputs.

# Methods

Let the continuous vector-valued action $$a_t \sim p(a_t \mid o_{\le t}, a_{<t})$$ generated by agent and $$o_t, r_t \sim p(o_t, r_t \mid o_{< t}, a_{< t})$$   in discrete time step $$t \in [1; T]$$. The goal is to develop an agent that maximizes the expected sum of rewards $$E_p(\sum^t_{t=1}r_t)$$. 

                        

## Latent dynamics

Dreamer uses a ***latent dynamics*** model with three components.

Representation model $$p(s_t \mid s_{t-1}, a_{t-1}, o_t)$$: Encodes observations and action to create continuous vector-valued model state $$s_t$$ with Markovian transitions. → embedding observation to latent state.

Transition model $$q(s_t \mid s_{t-1}, a_{t-1})$$: Predict future model states without seeing the corresponding observations that will later cause them. → predict next state based on previous action. → observation 정보 $$o_t$$ 없이 다음 state를 예측한다!

Reward model $q(r_t \mid s_t)$: Predict rewards given the $s_t$.

즉, Dreamer는 latent space 상에서 현재 상황을 요약 → 미래 상황을 예측 → 보상(행동) 예측 순으로 실행됨.

<img src="/assets/images/CheVi/figure1.png" class="img-medium" alt="Figure 1">
Comparison between (a) dataset of past experience, (b) training in imagination and (c) inference acting.

## Learning Behaviors by Latent Imagination

### Attribute of Imagination environment

The latent dynamics define fully observed Markov decision process (MDP) with compressed latent state $s_t$. Let $\tau$ as time index in imagined latent. The beggining of imagined trajectories start at the true model states $$s_t$$ of observation sequence from past experience.

즉 latent state $$s_t$$만으로 이전 시점의 모든 정보들이 압축적으로 담겨있기 때문에 markov 상태가 유지 가능하다.

### Action and value models

And imagined trajectories with finite horizon $H$, Dreamer uses an actor critic approach to learn behaviors that consider rewards beyond the horizon with action model and value models.

**Action model:** $$a_{\tau} \sim q_{\phi}(a_{\tau} \mid s_{\tau})$$

**Value model:** $$v_ψ(s_τ)≈\mathbb E_q(⋅\mid s_τ) \bigg [\underset{τ=t}{\overset{t+H}{\sum}}γ^{τ−t}r_τ \bigg ]$$

Thus, action model predict “best action” which high evaluated by value model while value model predict “value of action” with imagination beyond horizon.

They use dense neural network for the action and value model with parameter $$\phi$$ and  $$\psi$$. And the action model outputs a tanh-transformed Gaussian with sufficient statistic predicted by the neural network.

$$
a_{\tau} = \mathrm{tanh}(\mu_{\phi}(s_{\tau}) + \sigma_{\phi}(s_{\tau}) \epsilon), \ \ \epsilon \sim \mathrm{Normal(0, I)}
$$

$$\mu_{\phi}(s_{\tau})$$: Mean of “best action”s value

$$\sigma_{\phi}(s_{\tau})$$ : How far we go? → 시점 $$\tau$$에서 얼마나 랜덤하게 행동할지

### Value estimation

To learn the action and value model, we have to estimate the sate values of imagined trajectories $$\{ s_{\tau}, a_{\tau}, r_{\tau} \}^{t+H}_{\tau=t}$$.
